#!/bin/bash

# LLM Manager Script for llama.cpp (Local Version)
# Generated by Ansible - Do not edit directly

LLAMA_CPP_DIR="{{ ansible_env.HOME }}/llm-workspace/llama.cpp"
MODELS_DIR="{{ ansible_env.HOME }}/llm-workspace/models"
LOGS_DIR="{{ ansible_env.HOME }}/llm-workspace/logs"
SERVER_HOST="{{ llm_settings.server_host }}"
SERVER_PORT="{{ llm_settings.server_port }}"
DEFAULT_MODEL="{{ llm_settings.default_model }}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

list_models() {
    print_status "Available models in $MODELS_DIR:"
    echo ""
    if ls "$MODELS_DIR"/*.gguf >/dev/null 2>&1; then
        ls -lh "$MODELS_DIR"/*.gguf | awk '{print "  " $9 " (" $5 ")"}'
    else
        print_warning "No models found. Download models to: $MODELS_DIR"
    fi
}

download_model() {
    local model_name="$1"
    
    case "$model_name" in
        tinyllama)
            local url="{{ models.tinyllama.url }}"
            local filename="{{ models.tinyllama.filename }}"
            ;;
        phi3)
            local url="{{ models.phi3_mini.url | default('https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf') }}"
            local filename="{{ models.phi3_mini.filename | default('phi-3-mini-4k-instruct-q4.gguf') }}"
            ;;
        *)
            print_error "Unknown model: $model_name"
            print_status "Available models: tinyllama, phi3"
            return 1
            ;;
    esac
    
    print_status "Downloading $model_name..."
    curl -L -o "$MODELS_DIR/$filename" "$url"
    print_status "Model downloaded: $filename"
}

run_inference() {
    local model_file="$1"
    local prompt="$2"
    
    if [ ! -f "$model_file" ] && [ ! -f "$MODELS_DIR/$model_file" ]; then
        # Try default model
        model_file="$MODELS_DIR/{{ models.tinyllama.filename }}"
        if [ ! -f "$model_file" ]; then
            print_error "Model not found. Please download a model first."
            return 1
        fi
    elif [ -f "$MODELS_DIR/$model_file" ]; then
        model_file="$MODELS_DIR/$model_file"
    fi
    
    print_status "Running inference with $(basename $model_file)..."
    
    "$LLAMA_CPP_DIR/build/bin/llama-cli" \
        -m "$model_file" \
        -p "$prompt" \
        -n 512 \
        --temp 0.7 \
        --top-p 0.95 \
        --repeat-penalty 1.1 \
        -ngl {{ llm_settings.n_gpu_layers }} \
        -t {{ llm_settings.n_threads }}
}

start_server() {
    local model_file="${1:-$MODELS_DIR/{{ models.tinyllama.filename }}}"
    
    if [ ! -f "$model_file" ] && [ -f "$MODELS_DIR/$model_file" ]; then
        model_file="$MODELS_DIR/$model_file"
    fi
    
    if [ ! -f "$model_file" ]; then
        print_error "Model not found: $model_file"
        return 1
    fi
    
    print_status "Starting server with $(basename $model_file)..."
    print_status "Server will be available at http://$SERVER_HOST:$SERVER_PORT"
    
    launchctl load -w "$HOME/Library/LaunchAgents/com.llama.server.plist" 2>/dev/null || {
        # Fallback to manual start if launchd fails
        "$LLAMA_CPP_DIR/build/bin/llama-server" \
            -m "$model_file" \
            --host "$SERVER_HOST" \
            --port "$SERVER_PORT" \
            -c {{ llm_settings.context_size }} \
            -ngl {{ llm_settings.n_gpu_layers }} \
            --parallel 4 \
            --cont-batching \
            --flash-attn \
            > "$LOGS_DIR/server.log" 2>&1 &
        
        echo $! > "$LOGS_DIR/server.pid"
        print_status "Server started with PID $(cat $LOGS_DIR/server.pid)"
    }
}

stop_server() {
    print_status "Stopping LLM server..."
    
    # Try launchd first
    launchctl unload "$HOME/Library/LaunchAgents/com.llama.server.plist" 2>/dev/null || {
        # Fallback to PID file
        if [ -f "$LOGS_DIR/server.pid" ]; then
            kill $(cat "$LOGS_DIR/server.pid") 2>/dev/null
            rm "$LOGS_DIR/server.pid"
        fi
        
        # Last resort - kill by process name
        pkill -f llama-server 2>/dev/null
    }
    
    print_status "Server stopped"
}

server_status() {
    if curl -s "http://$SERVER_HOST:$SERVER_PORT/health" | grep -q "ok"; then
        print_status "Server is running at http://$SERVER_HOST:$SERVER_PORT"
        return 0
    else
        print_warning "Server is not running"
        return 1
    fi
}

test_setup() {
    print_status "Testing LLM setup..."
    
    # Check build
    if [ ! -f "$LLAMA_CPP_DIR/build/bin/llama-cli" ]; then
        print_error "llama-cli not found. Please run ansible playbook to build."
        return 1
    fi
    print_status "✅ llama.cpp is built"
    
    # Check models
    if ! ls "$MODELS_DIR"/*.gguf >/dev/null 2>&1; then
        print_warning "No models found"
        return 1
    fi
    print_status "✅ Models found"
    
    # Check server
    if server_status >/dev/null 2>&1; then
        print_status "✅ Server is running"
        
        # Test inference
        response=$(curl -s -X POST "http://$SERVER_HOST:$SERVER_PORT/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d '{"messages":[{"role":"user","content":"Say hello"}],"max_tokens":10}')
        
        if echo "$response" | grep -q "choices"; then
            print_status "✅ Inference working"
        else
            print_error "❌ Inference failed"
        fi
    else
        print_warning "Server not running. Start with: $0 server start"
    fi
    
    print_status "Test complete!"
}

show_help() {
    echo "LLM Manager for llama.cpp (Local Version)"
    echo ""
    echo "Usage: $0 <command> [arguments]"
    echo ""
    echo "Commands:"
    echo "  list                      - List available models"
    echo "  download <model>          - Download a model (tinyllama, phi3)"
    echo "  inference <prompt>        - Run inference with default model"
    echo "  server start [model]      - Start API server"
    echo "  server stop              - Stop API server"
    echo "  server status            - Check server status"
    echo "  test                     - Test the LLM setup"
    echo "  help                     - Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0 list"
    echo "  $0 download tinyllama"
    echo "  $0 inference \"What is the capital of France?\""
    echo "  $0 server start"
    echo "  $0 server status"
}

# Main logic
case "$1" in
    list)
        list_models
        ;;
    download)
        download_model "$2"
        ;;
    inference)
        shift
        run_inference "" "$*"
        ;;
    server)
        case "$2" in
            start)
                start_server "$3"
                ;;
            stop)
                stop_server
                ;;
            status)
                server_status
                ;;
            *)
                show_help
                ;;
        esac
        ;;
    test)
        test_setup
        ;;
    help|--help|-h|"")
        show_help
        ;;
    *)
        print_error "Unknown command: $1"
        show_help
        exit 1
        ;;
esac