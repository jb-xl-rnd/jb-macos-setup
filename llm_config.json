{
  "models": {
    "tinyllama": {
      "path": "~/llm-workspace/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
      "description": "TinyLlama 1.1B Chat Q4 - Lightweight model for basic tasks",
      "size": "637MB",
      "context_size": 2048,
      "parameters": "1.1B",
      "quantization": "Q4_K_M",
      "recommended_settings": {
        "temperature": 0.7,
        "top_p": 0.95,
        "n_gpu_layers": -1,
        "n_threads": 8
      }
    },
    "phi3-mini": {
      "path": "~/llm-workspace/models/phi-3-mini-4k-instruct.Q4_K_M.gguf",
      "description": "Phi-3 Mini 4K - More capable reasoning model",
      "size": "~2GB",
      "context_size": 4096,
      "parameters": "3.8B",
      "quantization": "Q4_K_M",
      "recommended_settings": {
        "temperature": 0.7,
        "top_p": 0.95,
        "n_gpu_layers": -1,
        "n_threads": 8
      }
    },
    "llama3-8b": {
      "path": "~/llm-workspace/models/llama-3-8b-instruct.Q4_K_M.gguf",
      "description": "Llama 3 8B - Full-featured instruction model",
      "size": "~4.5GB",
      "context_size": 8192,
      "parameters": "8B",
      "quantization": "Q4_K_M",
      "recommended_settings": {
        "temperature": 0.7,
        "top_p": 0.9,
        "n_gpu_layers": -1,
        "n_threads": 8
      }
    }
  },
  "server_defaults": {
    "host": "0.0.0.0",
    "port": 8080,
    "n_ctx": 2048,
    "n_batch": 512,
    "n_ubatch": 512,
    "flash_attn": true,
    "cont_batching": true,
    "metrics": true
  },
  "paths": {
    "llama_cpp": "~/llm-workspace/llama.cpp",
    "models_dir": "~/llm-workspace/models",
    "logs_dir": "~/llm-workspace/logs"
  }
}